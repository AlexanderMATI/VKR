{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTTxV7GxGdUg"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "!pip install scikit-learn\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "nltk.download('stopwords')\n",
        "import os,glob,re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii1j2od7agyQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsxyz2h8apuE"
      },
      "outputs": [],
      "source": [
        "rouge1_f1_5 = []\n",
        "rouge1_f1_10 = []\n",
        "rouge1_f1_15 = []\n",
        "rouge1_f1_20 = []\n",
        "\n",
        "bleu_5 = []\n",
        "bleu_10 = []\n",
        "bleu_15 = []\n",
        "bleu_20 = []\n",
        "\n",
        "sum_len_w = []\n",
        "sum_len_l = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywbshXsGGyON"
      },
      "outputs": [],
      "source": [
        "def extract_keywords(text, n_keywords):\n",
        "    # Обозначаем текст\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # Удаляем стоп-слова\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Вычислим частоту каждого слова\n",
        "    freq = Counter(tokens)\n",
        "\n",
        "    # Присваиваем баллы каждому слову в зависимости от частоты и положения\n",
        "    scores = {word: freq[word] * (i+1) for i, word in enumerate(tokens)}\n",
        "\n",
        "    #Отсортируем слова по количеству баллов и выбираем лучшие n_keywords\n",
        "    keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:n_keywords]\n",
        "\n",
        "    # Возвращаем лучшие ключевые слова\n",
        "    return [keyword[0] for keyword in keywords]\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "# Разделм краткое содержание на предложения, используя символ '.' в качестве разделителя\n",
        "    sentences = re.split(\"[.!?]\",summary)\n",
        "    \n",
        "    #Преобразуем каждое предложение в список слов\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Разделим абзац на слова, используя пробел в качестве разделителя\n",
        "    words = paragraph.split()\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hMwcWQYTHXBN"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/texts/London J\"\n",
        "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
        "      text = f.read().replace('\\n', '').replace('\\r', '')\n",
        "    i = 5\n",
        "    n_keywords = 10\n",
        "    while i < 21:\n",
        "\n",
        "        keywords = extract_keywords(text, n_keywords)\n",
        "\n",
        "        # Обобщим текст, используя основные ключевые слова\n",
        "        sentences = re.split(\"[.!?]\",text)\n",
        "        k = 0\n",
        "        summary = ''\n",
        "        for sentence in sentences:\n",
        "            for keyword in keywords:\n",
        "                if keyword in sentence.lower():\n",
        "                    summary += sentence.strip() + '. '\n",
        "                    k = k+1\n",
        "                    break\n",
        "        if (k == i):\n",
        "          break\n",
        "\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(summary, text)\n",
        "\n",
        "        reference_paragraph = text\n",
        "        reference_summary = summary_to_sentences(reference_paragraph)\n",
        "        predicted_paragraph = summary\n",
        "        predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "        score = sentence_bleu(reference_summary, predicted_summary)\n",
        "\n",
        "        match i:\n",
        "          case 5:\n",
        "            rouge1_f1_5.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_5.append(score)\n",
        "          case 10:\n",
        "            rouge1_f1_10.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_10.append(score) \n",
        "          case 15:\n",
        "            rouge1_f1_15.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_15.append(score)\n",
        "          case 20:\n",
        "            rouge1_f1_20.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_20.append(score)    \n",
        "        sum_len_l.append(len(summary))\n",
        "        sum_len_w.append(len(summary.split()))\n",
        "        i = i + 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hzEUgHyki13v"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "fields1 = [ 'sum_len_l','sum_len_w']\n",
        "rows1 = [sum_len_l, sum_len_w]\n",
        "\n",
        "fields2 = ['rouge1_f1_5','bleu5','rouge1_f1_10','bleu10','rouge1_f1_15','bleu_15','rouge1_f1_20','bleu_20']\n",
        "rows2 = [rouge1_f1_5,bleu_5,rouge1_f1_10,bleu_10,rouge1_f1_15,bleu_15,rouge1_f1_20,bleu_20]\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "export_data = zip_longest(*rows1, fillvalue = '')\n",
        "with open('len_short_LuhnLondon.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields1))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n",
        "\n",
        "\n",
        "export_data = zip_longest(*rows2, fillvalue = '')\n",
        "with open('metric_short_LuhnLondon.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields2))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}