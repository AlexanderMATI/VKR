{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-02-06T20:15:13.646002Z",
          "iopub.status.busy": "2023-02-06T20:15:13.645180Z",
          "iopub.status.idle": "2023-02-06T20:15:15.323507Z",
          "shell.execute_reply": "2023-02-06T20:15:15.322719Z",
          "shell.execute_reply.started": "2023-02-06T18:58:15.631794Z"
        },
        "papermill": {
          "duration": 1.702261,
          "end_time": "2023-02-06T20:15:15.323789",
          "exception": false,
          "start_time": "2023-02-06T20:15:13.621528",
          "status": "completed"
        },
        "tags": [],
        "id": "9XgO2uq3q4nv"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "!pip install rouge\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q-OEp4_MdT2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge1_f1_5 = []\n",
        "rouge1_f1_10 = []\n",
        "rouge1_f1_15 = []\n",
        "rouge1_f1_20 = []\n",
        "\n",
        "bleu_5 = []\n",
        "bleu_10 = []\n",
        "bleu_15 = []\n",
        "bleu_20 = []"
      ],
      "metadata": {
        "id": "5VE3gjrtdUeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(s1, s2):\n",
        "    \"\"\"\n",
        "    Вычисляем сходство между двумя предложениями на основе совпадения их слов.\n",
        "    \"\"\"\n",
        "    s1 = set(s1)\n",
        "    s2 = set(s2)\n",
        "    overlap = len(s1.intersection(s2))\n",
        "    if ((len(s1) + len(s2))==0):\n",
        "      return 0\n",
        "    return overlap / (len(s1) + len(s2))\n",
        "\n",
        "def summarize(text, num_sentences):\n",
        "    \"\"\"\n",
        "Суммируем данный текст с помощью алгоритма TextRank.    \"\"\"\n",
        "    # Разделите текст на предложения и слова\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "    # Удалите стоп-слова и знаки препинания\n",
        "    stop_words = set(stopwords.words('russian') + list(punctuation))\n",
        "    filtered_words = [[word for word in sentence if word not in stop_words] for sentence in words]\n",
        "\n",
        "    # Создайте словарь для хранения частот слов\n",
        "    word_freq = defaultdict(int)\n",
        "    for sentence in filtered_words:\n",
        "        for word in sentence:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "    # Рассчитайте баллы за предложения на основе частотности слов и сходства\n",
        "    sentence_scores = defaultdict(int)\n",
        "    for i, sentence in enumerate(filtered_words):\n",
        "        for word in sentence:\n",
        "            sentence_scores[i] += word_freq[word] / sum(word_freq.values())\n",
        "    for i, sentence in enumerate(filtered_words):\n",
        "        for j, other_sentence in enumerate(filtered_words):\n",
        "            if i == j:\n",
        "                continue\n",
        "            similarity = calculate_similarity(sentence, other_sentence)\n",
        "            sentence_scores[i] += similarity\n",
        "\n",
        "    # Отсортируйте предложения по баллам и выберите лучшие из них\n",
        "    top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
        "    top_sentences = [sentences[i] for i, score in top_sentences]\n",
        "\n",
        "    # Объедините верхние предложения в краткое изложение\n",
        "    summary = ' '.join(top_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def summary_to_sentences(summary):\n",
        "    # Разделите резюме на предложения, используя символ \".\" в качестве разделителя\n",
        "    sentences = re.split(\"[.!?]\",summary)\n",
        "    \n",
        "    # Преобразуйте каждое предложение в список слов\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Разделите абзац на слова, используя пробел в качестве разделителя\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "def listToString(s):\n",
        " \n",
        "    # инициализируйте пустую строку\n",
        "    str1 = \"\"\n",
        " \n",
        "    # перемещение по строке\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        " \n",
        "    # return string\n",
        "    return str1"
      ],
      "metadata": {
        "id": "Qmi5sZmDt5Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,glob,re\n",
        "path = \"/content/drive/My Drive/texts/short\"\n",
        "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
        "      text = f.read().replace('\\n', '').replace('\\r', '')\n",
        "# Создание сводки\n",
        "    num_sentences = 5\n",
        "    while num_sentences < 21:\n",
        "        sentences = re.split(\"[.!?]\",text)\n",
        "        text_len_l.append(len(text))\n",
        "        text_len_s.append(len(sentences))\n",
        "        text_len_w.append(len(listToString(sentences).split()))\n",
        "        summary = summarize(text, num_sentences)\n",
        "\n",
        "        from rouge import Rouge\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(summary, text)\n",
        "\n",
        "\n",
        "        from nltk.translate.bleu_score import sentence_bleu\n",
        "        reference_paragraph = text\n",
        "        reference_summary = summary_to_sentences(reference_paragraph)\n",
        "        predicted_paragraph = summary\n",
        "        predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "        score = sentence_bleu(reference_summary, predicted_summary)\n",
        "        sum_len_l.append(len(summary))\n",
        "        sum_len_w.append(len(summary.split()))\n",
        "        match num_sentences:\n",
        "          case 5:\n",
        "            rouge1_f1_5.append(scores[0]['rouge-1']['f'])\n",
        "            rouge2_f1_5.append(scores[0]['rouge-2']['f'])\n",
        "            rougel_f1_5.append(scores[0]['rouge-l']['f'])\n",
        "            bleu_5.append(score)\n",
        "          case 10:\n",
        "            rouge1_f1_10.append(scores[0]['rouge-1']['f'])\n",
        "            rouge2_f1_10.append(scores[0]['rouge-2']['f'])\n",
        "            rougel_f1_10.append(scores[0]['rouge-l']['f'])\n",
        "            bleu_10.append(score) \n",
        "          case 15:\n",
        "            rouge1_f1_15.append(scores[0]['rouge-1']['f'])\n",
        "            rouge2_f1_15.append(scores[0]['rouge-2']['f'])\n",
        "            rougel_f1_15.append(scores[0]['rouge-l']['f'])\n",
        "            bleu_15.append(score)\n",
        "          case 20:\n",
        "            rouge1_f1_20.append(scores[0]['rouge-1']['f'])\n",
        "            rouge2_f1_20.append(scores[0]['rouge-2']['f'])\n",
        "            rougel_f1_20.append(scores[0]['rouge-l']['f'])\n",
        "            bleu_20.append(score)    \n",
        "\n",
        "        num_sentences = num_sentences + 5\n"
      ],
      "metadata": {
        "id": "eYzQpiw0IO8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "fields1 = ['text_len_l', 'text_len_s','text_len_w', 'sum_len_l','sum_len_w']\n",
        "rows1 = [text_len_l, text_len_s, text_len_w, sum_len_l, sum_len_w]\n",
        "\n",
        "fields2 = ['rouge1_f1_5','rouge2_f1_5','rougel_f1_5','bleu5','rouge1_f1_10','rouge2_f1_10','rougel_f1_10','bleu10','rouge1_f1_15','rouge2_f1_15','rougel_f1_15','bleu_15','rouge1_f1_20','rouge2_f1_20','rougel_f1_20','bleu_20']\n",
        "rows2 = [rouge1_f1_5,rouge2_f1_5,rougel_f1_5,bleu_5,rouge1_f1_10,rouge2_f1_10,rougel_f1_10,bleu_10,rouge1_f1_15,rouge2_f1_15,rougel_f1_15,bleu_15,rouge1_f1_20,rouge2_f1_20,rougel_f1_20,bleu_20]\n",
        "\n",
        "from itertools import zip_longest\n",
        "#list1 = ['a', 'b', 'c', 'd', 'e']\n",
        "#list2 = ['f', 'g', 'i', 'j']\n",
        "#d = [list1, list2]\n",
        "export_data = zip_longest(*rows1, fillvalue = '')\n",
        "with open('len_textRank_new.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields1))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n",
        "\n",
        "export_data = zip_longest(*rows2, fillvalue = '')\n",
        "with open('metric_textRank_new.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields2))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n"
      ],
      "metadata": {
        "id": "N79GVhzuJw1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-06T20:15:48.681423Z",
          "iopub.status.busy": "2023-02-06T20:15:48.680280Z",
          "iopub.status.idle": "2023-02-06T20:15:57.396462Z",
          "shell.execute_reply": "2023-02-06T20:15:57.395761Z",
          "shell.execute_reply.started": "2023-02-06T19:01:18.088037Z"
        },
        "papermill": {
          "duration": 8.766006,
          "end_time": "2023-02-06T20:15:57.396638",
          "exception": false,
          "start_time": "2023-02-06T20:15:48.630632",
          "status": "completed"
        },
        "tags": [],
        "id": "brqgJZFsq4n-"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "print (\"text_len_l\", mean(text_len_l))\n",
        "print (\"text_len_s\", mean(text_len_s))\n",
        "print (\"text_len_w\", mean(text_len_w))\n",
        "print (\"sum_len_l\", mean(sum_len_l))\n",
        "print (\"sum_len_w\", mean(sum_len_w))\n",
        "\n",
        "print(\"rouge1_f1_5\", mean(rouge1_f1_5))\n",
        "print(\"rouge2_f1_5\", mean(rouge2_f1_5))\n",
        "print(\"rougel_f1_5\", mean(rougel_f1_5))\n",
        "print(\"bleu_5\", mean(bleu_5))\n",
        "\n",
        "print(\"rouge1_f1_10\", mean(rouge1_f1_10))\n",
        "print(\"rouge2_f1_10\", mean(rouge2_f1_10))\n",
        "print(\"rougel_f1_10\", mean(rougel_f1_10))\n",
        "print(\"bleu_10\", mean(bleu_10))\n",
        "\n",
        "print(\"rouge1_f1_15\", mean(rouge1_f1_15))\n",
        "print(\"rouge2_f1_15\", mean(rouge2_f1_15))\n",
        "print(\"rougel_f1_15\", mean(rougel_f1_15))\n",
        "print(\"bleu_15\", mean(bleu_15))\n",
        "\n",
        "print(\"rouge1_f1_20\", mean(rouge1_f1_20))\n",
        "print(\"rouge2_f1_20\", mean(rouge2_f1_20))\n",
        "print(\"rougel_f1_20\", mean(rougel_f1_20))\n",
        "print(\"bleu_20\", mean(bleu_20))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 63.141359,
      "end_time": "2023-02-06T20:16:10.903364",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-06T20:15:07.762005",
      "version": "2.2.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}