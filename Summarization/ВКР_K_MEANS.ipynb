{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4506f811"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from statistics import mean\n",
        "from heapq import nlargest\n",
        "from wordcloud import WordCloud\n",
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "import os,glob,re"
      ],
      "id": "4506f811"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qois9um3n3UB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "qois9um3n3UB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnlFHM_JoOsk"
      },
      "outputs": [],
      "source": [
        "def list_to_string(strings):\n",
        "    \"\"\"\n",
        "Преобразуем список строк в одну строку.\n",
        ":param strings: Список строк.\n",
        ":return: Одиночная строка.\n",
        "    \"\"\"\n",
        "    return \"\".join(strings)\n",
        "\n",
        "def text_to_word_list(text):\n",
        "    return text.split()\n",
        "\n",
        "def text_to_sentence_list(text):\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    return [sentence.split() for sentence in sent_tokenize(text)]"
      ],
      "id": "GnlFHM_JoOsk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbQRPr95oDXB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "rouge1_f1_5 = []\n",
        "rouge1_f1_10 = []\n",
        "rouge1_f1_15 = []\n",
        "rouge1_f1_20 = []\n",
        "\n",
        "bleu_5 = []\n",
        "bleu_10 = []\n",
        "bleu_15 = []\n",
        "bleu_20 = []\n",
        "\n",
        "sum_len_w = []\n",
        "sum_len_l = []"
      ],
      "id": "UbQRPr95oDXB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deb489cd"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/My Drive/texts/short\"\n",
        "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
        "      text: str = f.read().replace('\\n', '').replace('\\r', '')\n",
        "    \n",
        "    num_clusters = 5\n",
        "    while num_clusters < 21:\n",
        "        keep = string.ascii_letters + string.digits + \" \" + \".\"\n",
        "        text = list_to_string(text).translate(str.maketrans(\"\", \"\", string.punctuation.replace(\".\", \"\")))\n",
        "        text = text.lower()\n",
        "\n",
        "        text = sent_tokenize(text)\n",
        "\n",
        "        max_clusters = len(text)\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        X = vectorizer.fit_transform(text)\n",
        "            \n",
        "        # Определим оптимальное количество кластеров, используя метод elbow\n",
        "        sum_of_squared_distances = []\n",
        "        for num_clusters in range(1, max_clusters+1):\n",
        "            kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "            kmeans = kmeans.fit(X)\n",
        "            sum_of_squared_distances.append(kmeans.inertia_)\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        X = vectorizer.fit_transform(text)\n",
        "\n",
        "        # Выполним кластеризацию K-средних значений\n",
        "        kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "        clusters = kmeans.fit_predict(X)\n",
        "\n",
        "        # Отнесем каждое предложение к группе\n",
        "        labels = kmeans.labels_\n",
        "\n",
        "        # Создаем краткое изложение, выбрав по одному предложению из каждого кластера\n",
        "        summary = []\n",
        "        for i in range(num_clusters):\n",
        "            cluster_sentences = [text[j] for j in range(len(text)) if labels[j] == i]\n",
        "            summary.append(cluster_sentences[0])\n",
        "\n",
        "        len(summary)\n",
        "        paragraph = ' '.join(summary)\n",
        "\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(paragraph, list_to_string(text))\n",
        "\n",
        "        result = summary\n",
        "        summary_text = list_to_string(result)\n",
        "        predicted_summary = text_to_word_list(summary_text)\n",
        "        reference_summary = text\n",
        "        score = sentence_bleu(reference_summary, predicted_summary)\n",
        "\n",
        "        sum_len_l.append(len(summary))\n",
        "        sum_len_w.append(len(list_to_string(summary).split()))\n",
        "        match num_clusters:\n",
        "          case 5:\n",
        "            rouge1_f1_5.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_5.append(score)\n",
        "          case 10:\n",
        "            rouge1_f1_10.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_10.append(score) \n",
        "          case 15:\n",
        "            rouge1_f1_15.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_15.append(score)\n",
        "          case 20:\n",
        "            rouge1_f1_20.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_20.append(score)            \n",
        "        num_clusters = num_clusters+5"
      ],
      "id": "deb489cd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQgS6h7-rSUz"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "fields1 = [ 'sum_len_l','sum_len_w']\n",
        "rows1 = [sum_len_l, sum_len_w]\n",
        "\n",
        "fields2 = ['rouge1_f1_5','bleu5','rouge1_f1_10','bleu10','rouge1_f1_15''bleu_15','rouge1_f1_20','bleu_20']\n",
        "rows2 = [rouge1_f1_5,bleu_5,rouge1_f1_10,bleu_10,rouge1_f1_15,bleu_15,rouge1_f1_20,bleu_20]\n",
        "\n",
        "from itertools import zip_longest\n",
        "export_data = zip_longest(*rows1, fillvalue = '')\n",
        "with open('len_short_textRank.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields1))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n",
        "\n",
        "\n",
        "export_data = zip_longest(*rows2, fillvalue = '')\n",
        "with open('metric_short_textRank.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields2))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()"
      ],
      "id": "UQgS6h7-rSUz"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 34.107971,
      "end_time": "2023-02-09T14:33:20.222715",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-09T14:32:46.114744",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}