{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew0OMxThFd8Z"
      },
      "outputs": [],
      "source": [
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import gensim.downloader as api\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7lj0WZxmjf_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtM8aRKaGRVl"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Вычислить косинусное сходство между двумя векторами.\"\"\"\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "def build_similarity_matrix(sentences, threshold=0.1):\n",
        "    \"\"\"Строим матрицу сходства предложений.\"\"\"\n",
        "    n = len(sentences)\n",
        "    similarity_matrix = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                continue\n",
        "            similarity = cosine_similarity(sentences[i], sentences[j])\n",
        "            if similarity > threshold:\n",
        "                similarity_matrix[i][j] = similarity\n",
        "    return similarity_matrix\n",
        "\n",
        "def lexrank(sentences, threshold=0.1, damping_factor=0.85, max_iter=100):\n",
        "    \"\"\"Рассчитываем баллы LexRank для предложений.\"\"\"\n",
        "    #строим матрицу подобия\n",
        "    similarity_matrix = build_similarity_matrix(sentences, threshold=threshold)\n",
        "\n",
        "    # Нормализуем строки матрицы подобия\n",
        "    row_sums = similarity_matrix.sum(axis=1, keepdims=True)\n",
        "    similarity_matrix = np.divide(similarity_matrix, row_sums)\n",
        "\n",
        "    n = len(sentences)\n",
        "    scores = np.ones(n) / n\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        new_scores = np.zeros(n)\n",
        "        for j in range(n):\n",
        "            for k in range(n):\n",
        "                if similarity_matrix[k][j] > 0:\n",
        "                    new_scores[j] += similarity_matrix[k][j] * scores[k]\n",
        "            new_scores[j] = (1 - damping_factor) + damping_factor * new_scores[j]\n",
        "        if np.allclose(new_scores, scores):\n",
        "            break\n",
        "        scores = new_scores\n",
        "\n",
        "  \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSePdnXnGbE6"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    np.array([0.1, 0.2, 0.3]),\n",
        "    np.array([0.2, 0.3, 0.4]),\n",
        "    np.array([0.3, 0.4, 0.5]),\n",
        "    np.array([0.4, 0.5, 0.6])\n",
        "]\n",
        "scores = lexrank(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnDICezmGlNy",
        "outputId": "55ab98b3-c4f8-4812-f7a7-4b5471c90eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 1661.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgZekadLqTGM"
      },
      "outputs": [],
      "source": [
        "#Методы для преобразования текстов\n",
        "def summary_to_sentences(summary):\n",
        "    sentences = re.split(\"[.!?]\",summary)\n",
        "    \n",
        "\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "def listToString(s):\n",
        " \n",
        "    str1 = \"\"\n",
        "\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        " \n",
        "    return str1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljOfHpNNs1Ku"
      },
      "outputs": [],
      "source": [
        "text_len_l = []\n",
        "text_len_s = []\n",
        "text_len_w = []\n",
        "sum_len_w = []\n",
        "sum_len_l = []\n",
        "######\n",
        "rouge1_f1_5 = []\n",
        "rouge1_f1_10 = []\n",
        "rouge1_f1_15 = []\n",
        "rouge1_f1_20 = []\n",
        "\n",
        "rouge2_f1_5 = []\n",
        "rouge2_f1_10 = []\n",
        "rouge2_f1_15 = []\n",
        "rouge2_f1_20 = []\n",
        "\n",
        "rougel_f1_5 = []\n",
        "rougel_f1_10 = []\n",
        "rougel_f1_15 = []\n",
        "rougel_f1_20 = []\n",
        "\n",
        "bleu_5 = []\n",
        "bleu_10 = []\n",
        "bleu_15 = []\n",
        "bleu_20 = []\n",
        "\n",
        "summary_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HOT79bp2kn9",
        "outputId": "21ab1cfc-305a-4545-ab40-ea6f43e074ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "<ipython-input-5-40c977e566d3>:8: RuntimeWarning: invalid value encountered in true_divide\n",
            "  similarity_matrix = np.divide(similarity_matrix, row_sums)\n"
          ]
        }
      ],
      "source": [
        "import os,glob\n",
        "path = \"/content/drive/My Drive/texts/short2\"\n",
        "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
        "      text =f.read().replace('\\n', '').replace('\\r', '')\n",
        "    num_sentenses = 10\n",
        "    while num_sentenses < 41:\n",
        "        # Разбиваем текст на предложения\n",
        "        sentences = re.split(\"[.!?]\",text)\n",
        "        text_len_l.append(len(text))\n",
        "        text_len_s.append(len(sentences))\n",
        "        text_len_w.append(len(listToString(sentences).split()))\n",
        "        # Генерируем вложения для каждого предложения\n",
        "        sentence_embeddings = []\n",
        "        for sentence in sentences:\n",
        "            words = sentence.split()\n",
        "            embeddings = [model[word] for word in words if word in model.key_to_index ]\n",
        "            if len(embeddings) > 0:\n",
        "                sentence_embeddings.append(np.mean(embeddings, axis=0))\n",
        "        sentence_embeddings\n",
        "        # Рассчитываем баллы LexRank для предложений\n",
        "        scores = lexrank(sentence_embeddings)\n",
        "\n",
        "        # сортируем предложения по баллам и получите n лучших предложений в качестве резюме\n",
        "        top_sentences = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sentenses]\n",
        "        summary = [sentences[i].strip() for i in top_sentences]\n",
        "        summ= listToString(summary)\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(summ, text)\n",
        "        reference_paragraph = text\n",
        "        reference_summary = summary_to_sentences(reference_paragraph)\n",
        "        predicted_paragraph = summ\n",
        "        predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "        score = sentence_bleu(reference_summary, predicted_summary)\n",
        "        sum_len_w.append(len(listToString(summary).split()))\n",
        "        sum_len_l.append(len(summ))\n",
        "        summary_list.append(summ)\n",
        "        match num_sentenses:\n",
        "          case 10:\n",
        "            rouge1_f1_5.append(scores[0]['rouge-1']['f'])\n",
        "\n",
        "            bleu_5.append(score)\n",
        "          case 20:\n",
        "            rouge1_f1_10.append(scores[0]['rouge-1']['f'])\n",
        "\n",
        "            bleu_10.append(score) \n",
        "          case 30:\n",
        "            rouge1_f1_15.append(scores[0]['rouge-1']['f'])\n",
        "\n",
        "            bleu_15.append(score)\n",
        "          case 40:\n",
        "            rouge1_f1_20.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_20.append(score)    \n",
        "\n",
        "        num_sentenses = num_sentenses+10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uwxlRx6x4Kz"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "fields1 = ['text_len_l', 'text_len_s','text_len_w', 'sum_len_l','sum_len_w']\n",
        "rows1 = [text_len_l, text_len_s, text_len_w, sum_len_l, sum_len_w]\n",
        "\n",
        "fields2 = ['rouge1_f1_5','bleu5','rouge1_f1_10','bleu10','rouge1_f1_15','bleu_15','rouge1_f1_20','bleu_20','summary_list']\n",
        "rows2 = [rouge1_f1_5,bleu_5,rouge1_f1_10,bleu_10,rouge1_f1_15,bleu_15,rouge1_f1_20,bleu_20,summary_list]\n",
        "\n",
        "from itertools import zip_longest\n",
        "#list1 = ['a', 'b', 'c', 'd', 'e']\n",
        "#list2 = ['f', 'g', 'i', 'j']\n",
        "#d = [list1, list2]\n",
        "export_data = zip_longest(*rows1, fillvalue = '')\n",
        "with open('len_LexRank.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields1))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n",
        "\n",
        "\n",
        "export_data = zip_longest(*rows2, fillvalue = '')\n",
        "with open('metric_LexRank.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields2))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKzyO16Tz2qV"
      },
      "outputs": [],
      "source": [
        "from statistics import mean\n",
        "print (\"text_len_l\", mean(text_len_l))\n",
        "print (\"text_len_s\", mean(text_len_s))\n",
        "print (\"text_len_w\", mean(text_len_w))\n",
        "print (\"sum_len_l\", mean(sum_len_l))\n",
        "print (\"sum_len_w\", mean(sum_len_w))\n",
        "\n",
        "print(\"rouge1_f1_5\", mean(rouge1_f1_5))\n",
        "print(\"bleu_5\", mean(bleu_5))\n",
        "\n",
        "print(\"rouge1_f1_10\", mean(rouge1_f1_10))\n",
        "print(\"bleu_10\", mean(bleu_10))\n",
        "\n",
        "print(\"rouge1_f1_15\", mean(rouge1_f1_15))\n",
        "print(\"bleu_15\", mean(bleu_15))\n",
        "\n",
        "print(\"rouge1_f1_20\", mean(rouge1_f1_20))\n",
        "print(\"bleu_20\", mean(bleu_20))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}