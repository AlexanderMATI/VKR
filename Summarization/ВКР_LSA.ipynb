{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQCAXTnXNp-F"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "from rouge import Rouge \n",
        "import nltk\n",
        "import nltk.translate.bleu_score as bleu\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AGRmKaUDVIMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_len_l = []\n",
        "text_len_s = []\n",
        "text_len_w = []\n",
        "sum_len_w = []\n",
        "sum_len_l = []\n",
        "######\n",
        "rouge1_f1_5 = []\n",
        "rouge1_f1_10 = []\n",
        "rouge1_f1_15 = []\n",
        "rouge1_f1_20 = []\n",
        "\n",
        "rouge2_f1_5 = []\n",
        "rouge2_f1_10 = []\n",
        "rouge2_f1_15 = []\n",
        "rouge2_f1_20 = []\n",
        "\n",
        "rougel_f1_5 = []\n",
        "rougel_f1_10 = []\n",
        "rougel_f1_15 = []\n",
        "rougel_f1_20 = []\n",
        "\n",
        "bleu_5 = []\n",
        "bleu_10 = []\n",
        "bleu_15 = []\n",
        "bleu_20 = []\n",
        "\n",
        "sum_list = []"
      ],
      "metadata": {
        "id": "_pxRvbxPVGrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_to_sentences(summary):\n",
        "    # Разделяем резюме на предложения, используя символ \".\" в качестве разделителя\n",
        "    sentences = re.split(\"[.!?]\", summary)\n",
        "    \n",
        "    # Преобразуем каждое предложение в список слов\n",
        "    sentence_lists = [sentence.split() for sentence in sentences]\n",
        "    \n",
        "    return sentence_lists\n",
        "\n",
        "def paragraph_to_wordlist(paragraph):\n",
        "    # Разделим абзац на слова, используя пробел в качестве разделителя\n",
        "    words = paragraph.split()\n",
        "    return words\n",
        "\n",
        "def listToString(s):\n",
        " \n",
        "    # инициализируйте пустую строку\n",
        "    str1 = \"\"\n",
        " \n",
        "    # перемещение по строке\n",
        "    for ele in s:\n",
        "        str1 += ele\n",
        " \n",
        "    # возвращаемая строка\n",
        "    return str1"
      ],
      "metadata": {
        "id": "AVBqoYaXUzlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,glob,re\n",
        "path = \"/content/drive/My Drive/texts/London J\"\n",
        "for filename in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
        "      text = f.read().replace('\\n', '').replace('\\r', '')\n",
        "\n",
        "    num_sentences = 5\n",
        "    while num_sentences < 21:\n",
        "        sentences = re.split(\"[.!?]\",text)\n",
        "        text_len_l.append(len(text))\n",
        "        text_len_s.append(len(sentences))\n",
        "        text_len_w.append(len(listToString(sentences).split()))\n",
        "        vectorizer = TfidfVectorizer(stop_words=list('russian'))\n",
        "        X = vectorizer.fit_transform([text])\n",
        "        lsa = TruncatedSVD(num_sentences, algorithm='randomized', n_iter=1000) \n",
        "        lsa.fit(X)\n",
        "        sentences = re.split(\"[.!?]\",text)\n",
        "        important_sentences = np.argsort(np.abs(lsa.components_[0]))[::-1]\n",
        "\n",
        "        # Убедимся, что индексы в important_sentences находятся в пределах допустимых индексов для предложений\n",
        "        valid_indices = [i for i in important_sentences if i < len(sentences)]\n",
        "\n",
        "        # Извлечемм два наиболее важных предложения на основе допустимых индексов\n",
        "        summary_sentences = [sentences[i].strip() for i in valid_indices[:num_sentences]]\n",
        "\n",
        "        # Если допустимых индексов недостаточно, дополними сводку пустыми строками\n",
        "        while len(summary_sentences) < 2:\n",
        "            summary_sentences.append('')\n",
        "\n",
        "        summary = '. '.join(summary_sentences) + '.'\n",
        "\n",
        "        rouge = Rouge()\n",
        "        scores = rouge.get_scores(summary, text)\n",
        "        sum_len_l.append(len(summary))\n",
        "        sum_len_w.append(len(summary.split()))\n",
        "        from nltk.translate.bleu_score import sentence_bleu\n",
        "        reference_paragraph = text\n",
        "        reference_summary = summary_to_sentences(reference_paragraph)\n",
        "        predicted_paragraph = summary\n",
        "        predicted_summary = paragraph_to_wordlist(predicted_paragraph)\n",
        "\n",
        "        score = sentence_bleu(reference_summary, predicted_summary)\n",
        "        match num_sentences:\n",
        "          case 5:\n",
        "            rouge1_f1_5.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_5.append(score)\n",
        "          case 10:\n",
        "            rouge1_f1_10.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_10.append(score) \n",
        "          case 15:\n",
        "            rouge1_f1_15.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_15.append(score)\n",
        "          case 20:\n",
        "            rouge1_f1_20.append(scores[0]['rouge-1']['f'])\n",
        "            bleu_20.append(score)    \n",
        "\n",
        "        sum_list.append(summary)\n",
        "        num_sentences = num_sentences + 5"
      ],
      "metadata": {
        "id": "mqumimk7N9G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "fields1 = ['text_len_l', 'text_len_s','text_len_w', 'sum_len_l','sum_len_w']\n",
        "rows1 = [text_len_l, text_len_s, text_len_w, sum_len_l, sum_len_w]\n",
        "\n",
        "fields2 = ['rouge1_f1_5','bleu5','rouge1_f1_10','bleu10','rouge1_f1_15','bleu_15','rouge1_f1_20','bleu_20','sum_list']\n",
        "rows2 = [rouge1_f1_5,bleu_5,rouge1_f1_10,bleu_10,rouge1_f1_15,bleu_15,rouge1_f1_20,bleu_20, sum_list]\n",
        "\n",
        "from itertools import zip_longest\n",
        "export_data = zip_longest(*rows1, fillvalue = '')\n",
        "with open('len_short_LSA.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields1))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()\n",
        "\n",
        "\n",
        "export_data = zip_longest(*rows2, fillvalue = '')\n",
        "with open('metric_short_LSA.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
        "      wr = csv.writer(myfile)\n",
        "      wr.writerow((fields2))\n",
        "      wr.writerows(export_data)\n",
        "myfile.close()"
      ],
      "metadata": {
        "id": "679ir0v7Wq5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}